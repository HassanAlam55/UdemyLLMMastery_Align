{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Mastery Align\n",
    "Code to Align LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: NVIDIA GeForce RTX 3080\n",
      "Tensor on GPU: tensor([1., 2., 3.], device='cuda:0')\n",
      "Device of tensor: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# # If requirements.txt is in the same directory as your notebook\n",
    "# # !pip install -r requirements.txt\n",
    "# import torch\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# # Check CUDA version\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "#     # Get the current device\n",
    "#     current_device = torch.cuda.current_device()\n",
    "    \n",
    "#     # Get the name of the current device\n",
    "#     print(f\"GPU device: {torch.cuda.get_device_name(current_device)}\")\n",
    "    \n",
    "#     # Test a small tensor operation on GPU\n",
    "#     x = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
    "#     print(f\"Tensor on GPU: {x}\")\n",
    "#     print(f\"Device of tensor: {x.device}\")\n",
    "# else:\n",
    "#     print(\"No GPU support detected. PyTorch is using CPU only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading fiels using python\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2580)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\urllib3\\response.py:748\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 748\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\http\\client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mSSLError\u001b[0m: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2580)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\urllib3\\response.py:872\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m    873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\urllib3\\response.py:759\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread operation timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;66;03m# SSL errors related to framing/MAC get wrapped and reraised here\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[1;31mSSLError\u001b[0m: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2580)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m file_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://ideami.com/llm_align\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownloading fiels using python\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(file_url)\n\u001b[0;32m      5\u001b[0m zipfile\u001b[38;5;241m.\u001b[39mZipFile(io\u001b[38;5;241m.\u001b[39mBytesIO(response\u001b[38;5;241m.\u001b[39mcontent))\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    266\u001b[0m         req,\n\u001b[0;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[0;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\requests\\models.py:828\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 828\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RequestsSSLError(e)\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;66;03m# Standard file-like object.\u001b[39;00m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;31mSSLError\u001b[0m: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2580)"
     ]
    }
   ],
   "source": [
    "# import requests, zipfile, io\n",
    "# file_url = 'https://ideami.com/llm_align'\n",
    "# print ('downloading fiels using python')\n",
    "# response = requests.get(file_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import ipdb\n",
    "import math\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32= True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.set_printoptions(threshold=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# trainign paramters\n",
    "batch_size = 1\n",
    "epochs = 3\n",
    "lr = 6e-5\n",
    "lr_warmup_steps = 100\n",
    "context = 1024\n",
    "alpha = 0.5 # weigiting for ORPO\n",
    "prompt_max_size = 512\n",
    "compile = False\n",
    "dtype = torch.bfloat16\n",
    "log_iters = 50\n",
    "\n",
    "# Hyperparamters\n",
    "dropout = 0.\n",
    "grad_clip = 1.0\n",
    "weight_decay =  0.0\n",
    "\n",
    "# DEvice\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('device: ', device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhassanalam\u001b[0m (\u001b[33mhassanalam-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Dropbox\\GithubRepo\\Udemy\\llm_align\\wandb\\run-20250402_215801-0eoglsnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hassanalam-self/aligntest/runs/0eoglsnm' target=\"_blank\">alignment-run2025_04_02_21_57_59</a></strong> to <a href='https://wandb.ai/hassanalam-self/aligntest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hassanalam-self/aligntest' target=\"_blank\">https://wandb.ai/hassanalam-self/aligntest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hassanalam-self/aligntest/runs/0eoglsnm' target=\"_blank\">https://wandb.ai/hassanalam-self/aligntest/runs/0eoglsnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logging\n",
    "project_name = 'aligntest'\n",
    "wandb_log = True\n",
    "wandb_project = project_name\n",
    "wandb_run_name = 'alignment-run'\n",
    "wandb_run_name = 'alignment-run'\n",
    "wandb_run_name = 'alignment-run' + datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering and tokenizing datast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c4df8aacbe4d679361719f86086e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39091 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a531738bdd24f0c950c4abd9b35f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/39091 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = './data/orpo_dataset9'\n",
    "dataset_name  = 'mlabonne/orpo-dpo-mix-40k'\n",
    "tokenizer_path = 'tokenizers/tok16384'\n",
    "checkpoint_dir = './models/'\n",
    "\n",
    "# Tokenizer Dataset\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set our interction template\n",
    "# tokenizer.chat_template = \"{% for message in messages%}{% if messge['role]==user'}\\n{{'<|user|> + messge['content] + eos_token'}}\\{}\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "else:\n",
    "    print ('filtering and tokenizing datast')\n",
    "    dataset = load_dataset(dataset_name, split='all')\n",
    "\n",
    "    # filter dataset\n",
    "    # dataset = dataset.filter(lambda r: r[\"source\"] != 'toxic-dpo-v0.2')\n",
    "\n",
    "    # shorten to     512\n",
    "    def filter_dataset(examples):\n",
    "        prompt_length = tokenizer.apply_chat_template(examples['chosen'][:-1], tokenize=True, add_generation_prompt=True, return_tensors='pt').size(-1)\n",
    "        # prompt_length = tokenizer.apply_chat_template(examples['chosen'][:-1], tokenize=True, add_generation_prompt=True, return_tensors='pt').size(-1)  \n",
    "\n",
    "\n",
    "        if prompt_length < prompt_max_size:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # preprocess and tokenize data\n",
    "    def preprocess_dataset (examples: Union [List, Dict]):\n",
    "        # select chosen field and releimante last itme\n",
    "        prompt = [tokenizer.apply_chat_template(item[:-1], tokenize=False, add_generation_prompt=True) for item in examples['chosen']]\n",
    "        chosen = [tokenizer.apply_chat_template(item, tokenize = False) for item in examples['chosen']]\n",
    "        rejected = [tokenizer.apply_chat_template(item, tokenize = False) for item in examples['rejected']]\n",
    "\n",
    "        # tokenizer\n",
    "        # Hugginf face Dict Format\n",
    "        # Fielsd: ids, type_id, tokesn, offsets, attention_masks, special_tokens_mask, overflowing\n",
    "        inputs = tokenizer(prompt, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        pos_labels = tokenizer(chosen, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        neg_labels = tokenizer(rejected, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        # add fields to input\n",
    "        inputs['positive_input_ids'] = pos_labels['input_ids']\n",
    "        inputs['postivie_attention_mask'] = pos_labels['attention_mask']\n",
    "\n",
    "        inputs['negative_input_id'] = neg_labels['input_ids']\n",
    "        inputs['negative_attenion_mask'] = neg_labels['attention_mask']\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dataset = dataset.filter (filter_dataset)\n",
    "\n",
    "    # pre process and dataset\n",
    "    # for multiprocessing : num_processing (32, os.cpu_count())\n",
    "    dataset = dataset.map(preprocess_dataset, batched = True, num_proc=1, remove_columns=dataset.column_names)\n",
    "\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "dataset = dataset.shuffle(seed).train_test_split(test_size=0.05)\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "#  Setup DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How can I calculate the productivity of a research study using the term \"prolific\" in a scientific research paper? Can you provide an example using the following data set in MATLAB code:\n",
      "prolific = [2 4 3 6 8 2 7 4 1 5];\n",
      "total_words = [250 500 350 700 900 200 800 400 150 600];\n",
      "I want to use the term \"prolific\" as a measure of productivity, where the higher the value, the more productive the study. How can I incorporate this into my analysis using MATLAB?</s> \n",
      "<|assistant|>\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n"
     ]
    }
   ],
   "source": [
    "it = iter (train_loader)\n",
    "batch = next(it)\n",
    "# print(tokenizer.decode(batch['input_ids']))[0]\n",
    "print(tokenizer.decode(batch['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no compile\n",
      "138.431232 M parameter\n"
     ]
    }
   ],
   "source": [
    "from llm import Llama, ModelArgs\n",
    "\n",
    "checkpoint = torch.load(os.path.join (checkpoint_dir, 'base_model.pt'))\n",
    "config = checkpoint.pop('config')\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    dim=config.hidden_size,\n",
    "    n_layers=config.num_hidden_layers,\n",
    "    n_heads = config.num_attention_heads,\n",
    "    n_kv_heads = config.num_key_value_heads,\n",
    "    vocab_size = config.vocab_size,\n",
    "    norm_eps= config.rms_norm_eps,\n",
    "    rope_theta=-config.rope_theta,\n",
    "    max_seq_len=context,\n",
    "    dropout=config.attention_dropout,\n",
    "    hidden_dim=config.intermediate_size,\n",
    "    attention_bias=config.attention_bias,\n",
    "    mlp_bias = config.mlp_bias\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    print ('[INFO] comiling model')\n",
    "    torch = torch.compile(model)\n",
    "else:\n",
    "    print('no compile')\n",
    "\n",
    "print (sum(p.numel() for p in model.parameters())/1e6, \"M parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr, betas = (0.9, 0.95), fused = device== 'cuda', weight_decay=weight_decay)\n",
    "\n",
    "num_traiing_steps = len (train_loader) * epochs\n",
    "\n",
    "def lr_lambda (current_step):\n",
    "    if current_step < lr_warmup_steps:\n",
    "        return float(current_step) /float(max( 1, lr_warmup_steps))\n",
    "    progress = float(current_step - lr_warmup_steps) /float(max(1, num_traiing_steps - lr_warmup_steps))\n",
    "    return max (0.0, 0.5 * (1.0 + math.cos(math.pi + float(0.5)* 2.0 * progress)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR (optimizer, lr_lambda, last_epoch = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logps(prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "    \n",
    "    mask = chosen_attention_mask[:, : -1] - prompt_attention_mask[:, 1:]\n",
    "    per_token_logps = torch.gather(logits[:, : -1, :].log_softmax(-1), dim = 2,\n",
    "                                    index= (mask * chosen_inputs[:,1:]).unsqueeze(2)).squeeeze(2)\n",
    "    \n",
    "    return torch.mul(per_token_logps, mask.to(dtype)).sum(dim = 1).to (dtype)/ mask.sum(dim = 1).to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "c_inputs: tensor([2, 4, 3, 2, 4, 2, 1, 0, 0, 0, 0])\n",
      "c_mask: tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "mask: tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0])\n",
      "c_inputs[1:]: tensor([4, 3, 2, 4, 2, 1, 0, 0, 0, 0])\n",
      "index: tensor([0, 0, 0, 4, 2, 1, 0, 0, 0, 0])\n",
      "torch.Size([10]) torch.Size([11, 5])\n",
      "index_expanded: tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "torch.Size([10, 1]) torch.Size([11, 5])\n",
      "gathered values: tensor([[0.2000],\n",
      "        [0.2000],\n",
      "        [0.2200],\n",
      "        [0.3200],\n",
      "        [0.8800],\n",
      "        [0.4100],\n",
      "        [0.2300],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2200]])\n",
      "per token_logps> tensor([0.2000, 0.2000, 0.2200, 0.3200, 0.8800, 0.4100, 0.2300, 0.2000, 0.2000,\n",
      "        0.2200])\n",
      "result: tensor([0.0000, 0.0000, 0.0000, 0.3200, 0.8800, 0.4100, 0.2300, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "f1: 1.840000033378601\n",
      "f2: 4\n",
      "fina: 0.46000000834465027\n"
     ]
    }
   ],
   "source": [
    "# p_mask =    torch.tensor ([1,1,1,1,0,0,0,0,0,0,0])\n",
    "# c_mask =    torch.tensor ([1,1,1,1,1,1,1,0,0,0,0])\n",
    "# c_inputs =  torch.tensor ([2,4,3,2,4,2,1,0,0,0,0])\n",
    "# print(f\"p_mask: {p_mask}\")\n",
    "# print(f\"c_inputs: {c_inputs}\")\n",
    "# print(f\"c_mask: {c_mask}\")\n",
    "# mask = c_mask[:-1] - p_mask[1:]\n",
    "# print(f\"mask: {mask}\")\n",
    "\n",
    "# logits = torch.tensor([\n",
    "#     [0.2, 0.4, 0.8, 0.1, 0.3],\n",
    "#     [0.2, 0.1, 0.5, 0.12, 0.31],\n",
    "#     [0.22, 0.44, 0.81, 0.13, 0.32],\n",
    "#     [0.29, 0.42, 0.84, 0.15, 0.32],\n",
    "#     [0.24, 0.48, 0.88, 0.17, 0.34],\n",
    "#     [0.21, 0.41, 0.81, 0.14, 0.33],\n",
    "#     [0.23, 0.43, 0.82, 0.16, 0.35],\n",
    "#     [0.2, 0.4, 0.8, 0.1, 0.3],\n",
    "#     [0.2, 0.1, 0.5, 0.12, 0.31],\n",
    "#     [0.22, 0.44, 0.81, 0.13, 0.32],\n",
    "#     [0.22, 0.44, 0.81, 0.13, 0.32]\n",
    "# ])\n",
    "\n",
    "# print(f\"c_inputs[1:]: {c_inputs[1:]}\")\n",
    "# index=(mask * c_inputs[1:])\n",
    "# print(f\"index: {index}\")\n",
    "\n",
    "# print (index.shape, logits.shape)\n",
    "\n",
    "# # Expand dimensions for correct gather shape\n",
    "# index_expanded = index.unsqueeze(1)\n",
    "# print(f\"index_expanded: {index_expanded}\")\n",
    "# print (index_expanded.shape, logits.shape)\n",
    "\n",
    "# # gather the values at the specified indicies\n",
    "# gathered_values = torch.gather(logits[:-1, :], dim = 1, index=index_expanded)\n",
    "# print (f'gathered values: {gathered_values}')\n",
    "\n",
    "# # remove extr dimnstion\n",
    "# per_token_logps = gathered_values.squeeze(1)\n",
    "# print (f'per token_logps> {per_token_logps}')\n",
    "\n",
    "# result = torch.mul (per_token_logps, mask)\n",
    "# print (f'result: {result}')\n",
    "# f1 = result.sum(dim = 0)\n",
    "# f2 = mask.sum(dim =0)\n",
    "# print (f'f1: {f1}')\n",
    "# print (f'f2: {f2}')\n",
    "# final = f1/f2\n",
    "# print (f'fina: {final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37136 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37136 [00:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'cua'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 9\u001b[0m\n\u001b[0;32m      8\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:271\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'positive_attention_mask'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining eroor, cleaning up\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# release GPU memory\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcua\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU Memory released\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\Py311UdemyLLMMasteryAlign\\Lib\\site-packages\\torch\\__init__.py:2003\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2000\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m   2001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 2003\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'cua'"
     ]
    }
   ],
   "source": [
    "# Alignment Training LookupError\n",
    "\n",
    "try: \n",
    "    for e in range (epochs):\n",
    "        for i, batch in tqdm (enumerate(train_loader), total=len(train_loader), dynamic_ncols=True):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            batch['positive_input_ids'] = batch['positive_input_ids'].to(device)\n",
    "            batch['positive_attention_mask'] = batch['positive_attention_mask'].to(device)\n",
    "            batch['negative_input_ids'] = batch['negative_input_ids'].to(device)\n",
    "            batch['negative_attention_mask'] = batch['negative_attention_mask'].to(device)\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "            neg_lables = batch['negative_input_ids'].clone()\n",
    "            neg_lables = batch['positive_input_ids'].clone()\n",
    "\n",
    "            # mask will have is prompat\n",
    "            mask = batch['attention_mask'] *batch['postitive_attention_mask']\n",
    "            pos_labels = pos_labels * mask.logical_not()\n",
    "\n",
    "            pos_labels[pos_labels==0] = tokenizer.pad_token_id\n",
    "            pos_labels[pos_labels == tokenizer.ad_token_id] = - 100\n",
    "            neg_lables[neg_lables == tokenizer.ad_token_id] = - 100\n",
    "\n",
    "            outputs_pos, loss_pos = model(batch['positive_input_ids'], pos_labels)\n",
    "            outputs_neg, loss_neg = model(batch['negative_input_ids'], neg_lables)\n",
    "\n",
    "            # calcute per toaken log probability needed to calcualte OPRO LOG ODDs\n",
    "\n",
    "            pos_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'],\n",
    "                chosen_inputs=batch['pos_input_ids'],\n",
    "                chosen_attention_mask=batch['positive_attention_masks'],\n",
    "                logits = outputs_pos\n",
    "            )\n",
    "\n",
    "            neg_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'],\n",
    "                chosen_inputs=batch['neg_input_ids'],\n",
    "                chosen_attention_mask=batch['negative_attention_masks'],\n",
    "                logits = outputs_neg\n",
    "\n",
    "            )\n",
    "\n",
    "            # calcuate oprs ods ratio\n",
    "            log_odds = (pos_prob - neg_prob) - (torch.log (1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n",
    "            # log_odds = (pos_prob - neg_prob) - (torch.log (1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n",
    "            sig_ratio = F.sigmoid(log_odds)\n",
    "            ratio = torch.log(sig_ratio)\n",
    "\n",
    "            loss = torch.mean  (loss_pos - (alpha*ratio).mean()).to(dtype=dtype)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print ('Training eroor, cleaning up')\n",
    "\n",
    "finally:\n",
    "    # release GPU memory\n",
    "    torch.cua.empty_cache()\n",
    "    print ('GPU Memory released')\n",
    "    sys.exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311UdemyLLMMasteryAlign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
